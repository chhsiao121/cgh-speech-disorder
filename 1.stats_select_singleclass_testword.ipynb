{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data pre-processing for phonological process error Binary Classification\n",
    "**Author:** chhsiao<br>\n",
    "**Date created:** 2022/09/22<br>\n",
    "**Last modified:** 2022/09/21<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "利用治療師的標記結果將資料做二分類處理，並且會將單音資料(syllable)重新命名為0和16兩類(e.g. 0=>某種phonological process, 16=>某種phonological process以外的音檔)<br>\n",
    "\n",
    "注意：<br>\n",
    "此程式預設字卡為wordcard_BACKING，需要更改字卡請更改**wordcard_dic**<br>\n",
    "使用前要更改的部份為治療師統計好的標記結果(.csv)的路徑,要計算的構音錯誤TARGET(e.g. 塞音化=>1),以及收音期間\n",
    "為什麼要把test的字卡跟0327和0820的字卡分開做是因為標記方式不同，如果test的字卡是塞音，個案就算念對，治療師也是把它標塞音"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import shutil\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET = \"塞音化\"\n",
    "WORDCARD = \"test\"\n",
    "DATE = \"0820_0916\"\n",
    "WORDCARD = \"wordcard_BACKING\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "STATPATH = '/D/TWCC/work/cgh_2022/jsonmv/stat/test/' + DATE+'/'+WORDCARD+'/compareST_'+DATE+'.csv'\n",
    "DATASET = \"binear_classification\"\n",
    "\n",
    "\n",
    "DATASETPATH = '/D/TWCC/work/cgh_2022/data/test/'+WORDCARD+'/use-three-ST-label-dataset_' + \\\n",
    "    DATE+'/' + DATASET+'/'\n",
    "\n",
    "SYLLABLEPATH = DATASETPATH + 'syllable/'\n",
    "WORDPATH = DATASETPATH + 'word/'\n",
    "\n",
    "DATAPATH = '/D/TWCC/data2022_ori/test/'+DATE+'/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_dict = {\n",
    "    \"塞音化\": 1,\n",
    "    \"母音化\": 2,\n",
    "    \"母音省略\": 3,\n",
    "    \"舌前音化\": 4,\n",
    "    \"舌根音化\": 5,\n",
    "    \"不送氣音化\": 6,\n",
    "    \"聲隨韻母\": 7,\n",
    "    \"邊音化\": 8,\n",
    "    \"齒間音\": 9,\n",
    "    \"子音省略\": 10,\n",
    "    \"擦音化\": 11,\n",
    "    \"介音省略\": 12,\n",
    "    \"塞擦音化\": 13,\n",
    "    \"複韻母省略\": 14,\n",
    "    \"其他\": 15,\n",
    "    \"正確\": 16,\n",
    "    \"雜訊無法辨識\": 17\n",
    "}\n",
    "class_name = [\"塞音化\", \"母音化\", \"母音省略\", \"舌前音化\", \"舌根音化\", \"不送氣音化\", \"聲隨韻母\",\n",
    "              \"邊音化\", \"齒間音\", \"子音省略\", \"擦音化\", \"介音省略\", \"塞擦音化\", \"複韻母省略\", \"其他\", \"正確\", \"雜訊無法辨識\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "BACKINGPATH = DATASETPATH + 'Backing/'\n",
    "save_Backing = 'class_number_wordcard_BACKING'+DATE+'_Backing'\n",
    "\n",
    "STOPINGPATH = DATASETPATH + 'Stoping/'\n",
    "save_Stoping = 'class_number_wordcard_BACKING'+DATE+'_Stoping'\n",
    "\n",
    "\n",
    "\n",
    "# backing_list0327 = [\n",
    "#     \"wordcard03_01_2.wav\",\n",
    "#     \"wordcard03_02_2.wav\",\n",
    "#     \"wordcard03_07_2.wav\",\n",
    "#     \"wordcard03_08_1.wav\"\n",
    "# ]\n",
    "# backing_list0820 = [\n",
    "#     \"wordcard03_01_2.wav\",\n",
    "#     \"wordcard03_02_2.wav\",\n",
    "#     \"wordcard03_07_2.wav\",\n",
    "#     \"wordcard03_08_1.wav\"\n",
    "# ]\n",
    "# stoping_list0327 = [\n",
    "#     \"wordcard03_01_1.wav\",\n",
    "#     \"wordcard03_02_1.wav\",\n",
    "#     \"wordcard03_04_1.wav\",\n",
    "#     \"wordcard03_05_2.wav\",\n",
    "#     \"wordcard03_08_2.wav\",\n",
    "#     \"wordcard04_01_1.wav\"\n",
    "# ]\n",
    "# stoping_list0820 = [\n",
    "#     \"wordcard03_01_1.wav\",\n",
    "#     \"wordcard03_02_1.wav\",\n",
    "#     \"wordcard03_04_1.wav\",\n",
    "#     \"wordcard03_05_2.wav\",\n",
    "#     \"wordcard03_08_2.wav\",\n",
    "#     \"wordcard04_01_1.wav\",\n",
    "#     \"wordcard05_05_2.wav\",\n",
    "#     \"wordcard05_08_1.wav\"\n",
    "# ]\n",
    "\n",
    "\n",
    "classBackingChildNumber = {\n",
    "    \"非舌根音化\": 0,\n",
    "    \"舌根音化\": 0\n",
    "}\n",
    "classBackingAdultNumber = {\n",
    "    \"非舌根音化\": 0,\n",
    "    \"舌根音化\": 0\n",
    "}\n",
    "\n",
    "classStopingChildNumber = {\n",
    "    \"非塞音化\": 0,\n",
    "    \"塞音化\": 0\n",
    "}\n",
    "classStopingAdultNumber = {\n",
    "    \"非塞音化\": 0,\n",
    "    \"塞音化\": 0\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(TARGET == \"舌根音化\"):\n",
    "    classAdultNumber = classBackingAdultNumber\n",
    "    classChildNumber = classBackingChildNumber\n",
    "    ERRORPATH = BACKINGPATH\n",
    "    SAVEERRORNAME = save_Backing\n",
    "    if not os.path.exists(ERRORPATH):\n",
    "        os.makedirs(ERRORPATH)\n",
    "    # if(WORDCARD == \"data_0327word\"):\n",
    "    #     error_list = backing_list0327\n",
    "    # elif(WORDCARD== \"data_0820word\"):\n",
    "    #     error_list = backing_list0820\n",
    "elif(TARGET == \"塞音化\"):\n",
    "    classAdultNumber = classStopingAdultNumber\n",
    "    classChildNumber = classStopingChildNumber\n",
    "    ERRORPATH = STOPINGPATH\n",
    "    SAVEERRORNAME = save_Stoping\n",
    "    if not os.path.exists(ERRORPATH):\n",
    "        os.makedirs(ERRORPATH)\n",
    "    # if(WORDCARD == \"data_0327word\"):\n",
    "    #     error_list = stoping_list0327\n",
    "    # elif(WORDCARD == \"data_0820word\"):\n",
    "    #     error_list = stoping_list0820\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_dict[TARGET]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/D/TWCC/work/cgh_2022/jsonmv/stat/data_0327word/0401_0811/wordcard_BACKING/compareST_0401_0811.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_293724/3800281128.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mast\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mliteral_eval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf_stat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSTATPATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcase\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf_stat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mwordcard\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf_stat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/cgh-tf2/lib/python3.7/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/cgh-tf2/lib/python3.7/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/cgh-tf2/lib/python3.7/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/cgh-tf2/lib/python3.7/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/cgh-tf2/lib/python3.7/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/cgh-tf2/lib/python3.7/site-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/cgh-tf2/lib/python3.7/site-packages/pandas/io/parsers/base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m    227\u001b[0m             \u001b[0mmemory_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"memory_map\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"storage_options\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m             \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"encoding_errors\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"strict\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m         )\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/cgh-tf2/lib/python3.7/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    705\u001b[0m                 \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 707\u001b[0;31m                 \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    708\u001b[0m             )\n\u001b[1;32m    709\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/D/TWCC/work/cgh_2022/jsonmv/stat/data_0327word/0401_0811/wordcard_BACKING/compareST_0401_0811.csv'"
     ]
    }
   ],
   "source": [
    "from ast import literal_eval\n",
    "df_stat = pd.read_csv(STATPATH, index_col=0)\n",
    "\n",
    "for case in df_stat.columns.tolist(): \n",
    "    for wordcard in df_stat.index.tolist(): \n",
    "        if not df_stat[case][wordcard]: \n",
    "            pass\n",
    "        elif (df_stat[case][wordcard] != df_stat[case][wordcard]): \n",
    "            pass \n",
    "        else: \n",
    "            df_stat[case][wordcard] = literal_eval(df_stat[case][wordcard]) # convert to list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_stat' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_293724/3956981914.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_stat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'df_stat' is not defined"
     ]
    }
   ],
   "source": [
    "df_stat.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isWord(wordcard):\n",
    "    if(len(wordcard.split('_')) == 2):  # if the wordcard is WORD\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def isAdult(case):\n",
    "    if(case.split('_')[-1] == 'adult'):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def classPlusOne(ISADULT, TARGET):\n",
    "    if(ISADULT):\n",
    "        classAdultNumber[TARGET] += 1\n",
    "    else:\n",
    "        classChildNumber[TARGET] += 1\n",
    "\n",
    "\n",
    "def copy(src, destination):\n",
    "    if(os.path.exists(src)):\n",
    "        tmp = os.path.splitext(destination)[0]\n",
    "        tmpend = tmp.split('/')\n",
    "        index = tmp.find(tmpend[-1])\n",
    "        if not os.path.isdir(destination[:index]):\n",
    "            os.makedirs(destination[:index])\n",
    "        # print(src, destination)\n",
    "        # print(destination[:index])\n",
    "        shutil.copyfile(src, destination)\n",
    "    else:\n",
    "        print(f\"{src} not found\")\n",
    "\n",
    "for case in df_stat.columns.tolist():\n",
    "    for wordcard in df_stat.index.tolist():\n",
    "        # if the case is adult(2022.05.15.19.58.12_946516734_adult)\n",
    "        if(isAdult(case)):\n",
    "            tmpNewPath = ERRORPATH + 'adult/'\n",
    "            ISADULT = True\n",
    "        else:\n",
    "            tmpNewPath = ERRORPATH + 'child/'\n",
    "            ISADULT = False\n",
    "        ISWORD = isWord(wordcard)\n",
    "        if(ISWORD):#二分類不考慮詞的情況\n",
    "            continue\n",
    "        if not df_stat[case][wordcard]: # if the value is empty\n",
    "            pass\n",
    "        elif (df_stat[case][wordcard] != df_stat[case][wordcard]): # if the value is NaN\n",
    "            pass\n",
    "        else:\n",
    "\n",
    "            case_number = case.split('_')[1]  # 2022.04.01.13.12.28_477493581\n",
    "            # '/D/TWCC/data2022_ori/data_0327word/0401_0707/2022.06.16.09.33.28_1780799170_test/wordcard05_07_1.wav'\n",
    "            oldFilePath = DATAPATH+case+'/'+wordcard\n",
    "            # '/D/TWCC/data2022_ori/data_0327word/0401_0707/2022.06.16.09.33.28_1780799170_test/wordcard05_07_1'\n",
    "            filePath = os.path.splitext(oldFilePath)[0]\n",
    "            oldFileName = filePath.split('/')[-1]  # 'wordcard05_07_1'\n",
    "            if (17 in df_stat[case][wordcard]):\n",
    "                pass\n",
    "            # if ('fcdp' in wordcard):\n",
    "            #     pass\n",
    "            elif(16 in df_stat[case][wordcard]):\n",
    "                # if(wordcard in error_list):\n",
    "                #     newFileName = \"0_\" + str(case_number) + '_' + oldFileName.split('wordcard')[1] + '.wav'\n",
    "                #     classPlusOne(ISADULT, TARGET)\n",
    "                #     tmpNewPath = tmpNewPath + '0/'\n",
    "                # else:\n",
    "                newFileName = \"16_\" + str(case_number) + '_' + oldFileName.split('wordcard')[1] + '.wav'\n",
    "                classPlusOne(ISADULT, \"非\"+TARGET)\n",
    "                tmpNewPath = tmpNewPath + '16/'\n",
    "                if not os.path.isdir(tmpNewPath):\n",
    "                    os.makedirs(tmpNewPath)\n",
    "                newFilePath = os.path.join(tmpNewPath, newFileName)\n",
    "                # print(df_stat[case][wordcard], newFileName)\n",
    "                # print(f\"{oldFilePath} -> {newFilePath}\")\n",
    "                copy(oldFilePath, newFilePath)\n",
    "                # shutil.copyfile(oldFilePath, newFilePath)\n",
    "            elif(class_dict[TARGET] in df_stat[case][wordcard]):\n",
    "                newFileName = \"0_\" + str(case_number) + '_' + oldFileName.split('wordcard')[1] + '.wav'\n",
    "                classPlusOne(ISADULT, TARGET)\n",
    "                tmpNewPath = tmpNewPath + '0/'\n",
    "                if not os.path.isdir(tmpNewPath):\n",
    "                    os.makedirs(tmpNewPath)\n",
    "                newFilePath = os.path.join(tmpNewPath, newFileName)\n",
    "                # print(f\"{oldFilePath} -> {newFilePath}\")\n",
    "                # print(df_stat[case][wordcard], newFileName)\n",
    "                copy(oldFilePath, newFilePath)\n",
    "                # shutil.copyfile(oldFilePath, newFilePath)\n",
    "            elif(15 not in df_stat[case][wordcard]): #排除其他\n",
    "                newFileName = \"16_\" + str(case_number) + '_' + oldFileName.split('wordcard')[1] + '.wav'\n",
    "                classPlusOne(ISADULT, \"非\"+TARGET)\n",
    "                tmpNewPath = tmpNewPath + '16/'\n",
    "                if not os.path.isdir(tmpNewPath):\n",
    "                    os.makedirs(tmpNewPath)\n",
    "                newFilePath = os.path.join(tmpNewPath, newFileName)\n",
    "                # print(f\"{oldFilePath} -> {newFilePath}\")\n",
    "                copy(oldFilePath, newFilePath)\n",
    "                # shutil.copyfile(oldFilePath, newFilePath)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'非塞音化': 0, '塞音化': 0}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classChildNumber\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'非塞音化': 436, '塞音化': 62}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classAdultNumber\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving/D/TWCC/work/cgh_2022/data/test/wordcard_BACKING/use-three-ST-label-dataset_0820_0916/binear_classification/Stoping/class_number_0820_0916_Stopingchild.json\n"
     ]
    }
   ],
   "source": [
    "with open(ERRORPATH+SAVEERRORNAME+'child.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(classChildNumber, f, ensure_ascii=False, indent=4)\n",
    "    print(\"Saving\"+ERRORPATH+SAVEERRORNAME+'child.json')\n",
    "    f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving/D/TWCC/work/cgh_2022/data/test/wordcard_BACKING/use-three-ST-label-dataset_0820_0916/binear_classification/Stoping/class_number_0820_0916_Stopingadult.json\n"
     ]
    }
   ],
   "source": [
    "with open(ERRORPATH+SAVEERRORNAME+'adult.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(classAdultNumber, f, ensure_ascii=False, indent=4)\n",
    "    print(\"Saving\"+ERRORPATH+SAVEERRORNAME+'adult.json')\n",
    "    f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('cgh-tf2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d0d49c5b027e350f5dabb267e49fd20e6c5ba13ac4fa28295ca35f4c91aa4338"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
